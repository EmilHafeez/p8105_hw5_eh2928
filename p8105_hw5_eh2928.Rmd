---
title: "Homework 5, Module 7 (Iteration)"
author: "Emil Hafeez (eh2928)"
date: "11/11/2020"
output: github_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)

#Load relevant libraries
library(tidyverse)
library(purrr)

#Prep for neater output
knitr::opts_chunk$set(
	fig.width = 6, 
  fig.asp = .6,
  out.width = "90%"
)

#Theming with viridis, minimal
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1

First things first, read in the data, and describe it.

```{r reading}
homicide_df = 
    read.csv("./data/homicide/homicide.csv")

homicide_df %>% summary()
```

This raw dataset consists of `r homicide_df %>% nrow()` rows and `r homicide_df %>% ncol()` columns. Data are structured around individual-level homicide cases including an id number, and then details location, arrest/resolution information, and, often, demographic information regarding each victim. Several variables are not of a tidied data type (e.g. not a factor, or numeric, or date variable when this is prudent), and there is also missing data. 

```{r mutations, include = FALSE}
homicide_df = 
    homicide_df %>% 
  mutate(
    city_state = str_c(city, ", ", state),
    resolved = case_when(
      disposition == "Closed without arrest" ~ "unresolved",
      disposition == "Open/No arrest" ~ "unresolved",
      disposition == "Closed by arrest" ~ "resolved"
    )
  ) %>% 
  filter(city_state != "Tulsa, AL")
```

Total number of homicides in each city, and the number of unsolved homicides.
```{r homicide counter, message = F, warning = F, echo = FALSE}
aggregate_df = 
  homicide_df %>% 
  group_by(city_state) %>% 
  summarize(
    homicide_total = n(),
    homicide_unresolved = sum(resolved == "unresolved")
  ) %>% arrange(desc(homicide_total))
aggregate_df
```

Let's estimate the proportion of homicides that are unsolved in Baltimore, MD.
```{r prop.test, echo = FALSE}
baltimore_prop_test = 
  prop.test(
  aggregate_df %>% filter(city_state == "Baltimore, MD") %>%  pull(homicide_unresolved),
  aggregate_df %>% filter(city_state == "Baltimore, MD") %>%  pull(homicide_total)
    ) %>% 
  broom::tidy()

baltimore_prop_test %>% 
  mutate(
    estimate = round(estimate, digits = 2),
    conf.low = round(conf.low, digits = 2),
    conf.high = round(conf.high, digits = 2),
    baltimore_prop_result = str_c("Estimate: ", estimate, ", ", "Confidence Interval: (", conf.low, ",", conf.high, ")"),
  ) %>% pull(baltimore_prop_result)
```

Now run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. Do this within a “tidy” pipeline, making use of purrr::map, purrr::map2, list columns and unnest as necessary to create a tidy dataframe with estimated proportions and CIs for each city.

```{r iterate}
#iterate the prop tests and tidied tests functions over each state to get a resulting df
results_homicide_df =
  aggregate_df %>% 
  mutate(
    prop_tests = map2(.x = homicide_unresolved, .y = homicide_total, ~ prop.test(x = .x, n = .y)),
    tidied_tests = map(.x = prop_tests, ~broom::tidy(.x))
  ) %>% 
  select(-prop_tests) %>% 
  unnest(tidied_tests) %>% 
  select(city_state, estimate, conf.low, conf.high)
```

```{r ggplot}
results_homicide_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = .25, hjust = 1)) +
  scale_y_continuous(breaks = seq(0, 1, .05)) +
  labs(
    title = "Fig. 1: Estimates of the Proportion of Unsolved Homicides in US Cities",
    x = "City",
    y = "Proportion Estimate",
    caption = "Homework 5")
```

# Problem 2

Create a tidy dataframe containing data from all participants, including the subject ID, arm, and observations over time:

Start with a dataframe containing all file names; the list.files function will help
Iterate over file names and read in data for each subject using purrr::map and saving the result as a new variable in the dataframe
Tidy the result; manipulate file names to include control arm and subject ID, make sure weekly observations are “tidy”, and do any other tidying that’s necessary
Make a spaghetti plot showing observations on each subject over time, and comment on differences between groups.

```{r}
path_df =
  tibble(
    path = list.files(".data/lda")
  )
      str_c("lda", path)
  )
```




















