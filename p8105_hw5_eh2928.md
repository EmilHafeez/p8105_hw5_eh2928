Homework 5, Module 7 (Iteration)
================
Emil Hafeez (eh2928)
11/11/2020

# Problem 1

First things first, read in the data, and describe it.

``` r
homicide_df = 
    read.csv("./data/homicide/homicide.csv")

homicide_df %>% summary()
```

    ##      uid            reported_date       victim_last        victim_first      
    ##  Length:52179       Min.   : 20070101   Length:52179       Length:52179      
    ##  Class :character   1st Qu.: 20100318   Class :character   Class :character  
    ##  Mode  :character   Median : 20121216   Mode  :character   Mode  :character  
    ##                     Mean   : 20130899                                        
    ##                     3rd Qu.: 20150911                                        
    ##                     Max.   :201511105                                        
    ##                                                                              
    ##  victim_race         victim_age         victim_sex            city          
    ##  Length:52179       Length:52179       Length:52179       Length:52179      
    ##  Class :character   Class :character   Class :character   Class :character  
    ##  Mode  :character   Mode  :character   Mode  :character   Mode  :character  
    ##                                                                             
    ##                                                                             
    ##                                                                             
    ##                                                                             
    ##     state                lat             lon          disposition       
    ##  Length:52179       Min.   :25.73   Min.   :-122.51   Length:52179      
    ##  Class :character   1st Qu.:33.77   1st Qu.: -96.00   Class :character  
    ##  Mode  :character   Median :38.52   Median : -87.71   Mode  :character  
    ##                     Mean   :37.03   Mean   : -91.47                     
    ##                     3rd Qu.:40.03   3rd Qu.: -81.76                     
    ##                     Max.   :45.05   Max.   : -71.01                     
    ##                     NA's   :60      NA's   :60

This raw dataset consists of 52179 rows and 12 columns. Data are
structured around individual-level homicide cases including an id
number, and then details location, arrest/resolution information, and,
often, demographic information regarding each victim. Several variables
are not of a tidied data type (e.g. not a factor, or numeric, or date
variable when this is prudent), and there is also missing data.

Total number of homicides in each city, and the number of unsolved
homicides.

    ## # A tibble: 50 x 3
    ##    city_state       homicide_total homicide_unresolved
    ##    <chr>                     <int>               <int>
    ##  1 Chicago, IL                5535                4073
    ##  2 Philadelphia, PA           3037                1360
    ##  3 Houston, TX                2942                1493
    ##  4 Baltimore, MD              2827                1825
    ##  5 Detroit, MI                2519                1482
    ##  6 Los Angeles, CA            2257                1106
    ##  7 St. Louis, MO              1677                 905
    ##  8 Dallas, TX                 1567                 754
    ##  9 Memphis, TN                1514                 483
    ## 10 New Orleans, LA            1434                 930
    ## # … with 40 more rows

Let’s estimate the proportion of homicides that are unsolved in
Baltimore, MD.

    ## [1] "Estimate: 0.65, Confidence Interval: (0.63,0.66)"

Now run prop.test for each of the cities in your dataset, and extract
both the proportion of unsolved homicides and the confidence interval
for each. Do this within a “tidy” pipeline, making use of purrr::map,
purrr::map2, list columns and unnest as necessary to create a tidy
dataframe with estimated proportions and CIs for each city.

``` r
#iterate the prop tests and tidied tests functions over each state to get a resulting df
results_homicide_df =
  aggregate_df %>% 
  mutate(
    prop_tests = map2(.x = homicide_unresolved, .y = homicide_total, ~ prop.test(x = .x, n = .y)),
    tidied_tests = map(.x = prop_tests, ~broom::tidy(.x))
  ) %>% 
  select(-prop_tests) %>% 
  unnest(tidied_tests) %>% 
  select(city_state, estimate, conf.low, conf.high)
```

``` r
results_homicide_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = .25, hjust = 1)) +
  scale_y_continuous(breaks = seq(0, 1, .05)) +
  labs(
    title = "Fig. 1: Estimates of the Proportion of Unsolved Homicides in US Cities",
    x = "City",
    y = "Proportion Estimate",
    caption = "Homework 5")
```

<img src="p8105_hw5_eh2928_files/figure-gfm/ggplot-1.png" width="90%" />

# Problem 2

Create a tidy dataframe containing data from all participants, including
the subject ID, arm, and observations over time:

Start with a dataframe containing all file names; the list.files
function will help Iterate over file names and read in data for each
subject using purrr::map and saving the result as a new variable in the
dataframe Tidy the result; manipulate file names to include control arm
and subject ID, make sure weekly observations are “tidy”, and do any
other tidying that’s necessary Make a spaghetti plot showing
observations on each subject over time, and comment on differences
between groups.

``` r
path_df =
  tibble(
    path = list.files("./data/lda")
  ) %>% 
  mutate(path = str_c("./data/lda/", path),
         data = map(.x = path, ~read_csv(.x))
         ) %>% 
  unnest(data) %>% 
  mutate(
    path_separator = path
  ) %>% 
  separate(path_separator, into = c("arm", "subject_id"), sep = "_") %>% 
  mutate(
    arm = str_replace(arm, "./data/lda/con", "control"),
    arm = str_replace(arm, "./data/lda/exp", "experimental"),
    subject_id = str_replace(subject_id, ".csv", ""),
    subject_id = as.numeric(subject_id)
  ) %>% 
  arrange(path, arm, subject_id) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week_number",
    values_to = "observed_value"
  ) %>% 
  mutate(
    week_number = str_replace(week_number, "week_", ""),
    week_number = as.numeric((week_number)),
    arm = as.factor(arm),
    path = str_c(arm, "_", subject_id)
  )
```

``` r
ggp_lda =
path_df %>% 
  ggplot(aes(x = week_number, y = observed_value, group = path, color = arm)) + 
  geom_line(alpha = .5) +
  geom_smooth(
    aes(x = week_number, y = observed_value, color = arm, group = arm), 
        alpha = 1, inherit.aes = F, se = F) +
   scale_x_continuous(
    breaks = c(1, 2, 3, 4, 5, 6, 7, 8), 
    labels = c("Baseline", "2", "3", "4", "5", "6", "7", "Endline")) +
  labs(
    title = "Observed Values over Time, per Participant",
    x = "Week Number",
    y = "Value",
    caption = "Longitudinal Data Analysis Spaghetti Plot, Assignment 5") 
ggp_lda
```

    ## `geom_smooth()` using method = 'loess' and formula 'y ~ x'

<img src="p8105_hw5_eh2928_files/figure-gfm/plot-1.png" width="90%" />

The plot appears to show the experimental arm and the control arm start
from a similar baseline with respect to their observed values, but over
time the control arm stays approximately the same while the experimental
arm appears to increase. The smoothed averages for each group help
illustrate this point.

# Problem 3

Create the function

``` r
set.seed(27)
sim_t_test = function(sample_size = 30, mu = 0, sigma = 5) {
  
  sim_data = tibble(
    x = rnorm(n = sample_size, mean = mu, sd = sigma),
  )
  
  t_test = t.test(sim_data, mu = 0)

  sim_data %>% 
    summarize(
      mu_hat = mean(x),
      p_value = pull(broom::tidy(t_test), p.value)
    )
}
```

Generate 5000 datasets from the model
